### Transforming Product Development: How the GIST Framework Elevated My Career

Looking from the outside, one might assume I'm a big deal. I've launched many products starting with an idea and earned a big-ticket promotion to Google Engineering Director. My work made a difference to millions of people. But had you spoken to me a year before that fancy promotion, you'd have seen a frustrated Engineering Leader trying to offer value to their users (Googlers) by changing the approach to software development.

I tried everything I could. I brought Agile methodologies into Google, to my teams' dismay. I got [Marty Cagan](https://www.google.com/search?kgmid=/g/11f7snk27d&hl=en-US&q=Marty+Cagan&kgs=1ebe90503817a123&shndl=17&shem=ssim&source=sh/x/kp/osrp/m5/1), a product management thought leader, to inspire us to do Dual-Track Agile. Those helped a bit, but it wasn't enough.

Then, I discovered a game-changing product development framework. I know frameworks are often overhyped, but this one helped me drive a positive change in our approach to product development and helped me earn my promotion. I believe it can do the same for you. Let's dive in.

---

## **How did it start?**

My friend, an avid learner, frequently shares valuable insights. Sensing my frustration with traditional product roadmaps, he sent me an article titled: "[Why you should stop using product roadmaps and try the GIST Framework](https://itamargilad.com/gist-framework/)" by [Itamar Gilad](None).

> [**Why you should stop using product roadmaps and try the GIST Framework**](https://itamargilad.com/gist-framework/)

_I'm not affiliated with Itamar or GIST. I wish I were; then I'd be rich :) I'm sharing it because it has impacted me and my teams. I genuinely believe it can benefit yours, too._

## What's Wrong with Traditional Product Roadmaps

When I came across the article, I was frustrated by traditional product roadmaps' limitations, so I was keen to learn about alternatives.

Why were product roadmaps so ineffective? They never came to life. We would spend so much time and effort creating, debating, and agreeing on the next three years, and then, about six months into the execution. Things would change just enough to make most of the plan obsolete.

The other issue I experienced with product roadmaps was their lack of focus on user problems. [Marty Cagan](https://g.co/kgs/TW2RbeS) talks about "feature factories" or "output roadmaps."

## Discovering the GIST Framework

The article advocated for Outcome Roadmaps instead of eliminating product roadmaps and introducing the GIST framework. This was a great way to transform our current product development approach and offer our team a brighter future.

![[Picture](https://itamargilad.com/wp-content/uploads/2020/04/GIST-1024x698.jpg) from Itamar Gilad's site](https://miro.medium.com/0*tm8d6Q4Q7QNyIQTw)

GIST stands for Goals, Ideas, Steps, and Tasks:

- **Goals**: Align with the objectives in OKRs.

- **Ideas**: All ideas we can test to achieve those goals.

- **Steps**: Validation methods to test our ideas, ending with recalculating priorities using ICE scores.

- **Tasks**: Team tasks to execute each step.

## Implementing GIST

I was incredibly privileged to bring Itamar into (virtually) Google to train my teams. During the sessions, Itamar introduced us to some new tools. They were simple yet powerful, and each was matched with a helpful visual aid. He also refreshed our knowledge of known tools.

I found most of the tools, and I'm being frank here, **mind-blowing and eye-opening**. So much so that I'd present them in 80% of strategy and decision-making meetings over the next two years to get people on the same page, I bet you will do the same once you see their power.

Interested in the results GIST has unlocked for me? read in the link:

> [**Unlocking Agility at Scale: A Googler's Story**](https://code.likeagirl.io/unlocking-agility-at-scale-a-googlers-story-f0ea4c42d96c)

## Key Tools and Concepts

So, which tools are you wondering about?

### **OKRs**

I'll start with the least exciting and most surprising one. Most readers have heard about how Google uses OKRs and might think, like my peers, that there is nothing new here. About seven years into Google, I'd learned, written, commented on, fixed, and presented OKRs many times.

Okay, so no, it wasn't a new tool. It was, however, a potent reminder of how to write good OKRs with well-defined objectives (O's) and true key results (KR's) that are not simply "try this" or "launch this feature."

So, I found myself disagreeing with my peers' feedback that

> "The OKR part was a waste of time, we already know it."

This reminder was the catalyst for a significant overhaul that the leadership team implemented in creating OKRs and the standards we set for the triads - the cross-functional leadership team overseeing teams - in our organization.

**=> Takeaway**Even if you've used OKRs for a while, you probably do them wrong. Go back to the origins (or read Itamar's writeup about the topic) to see how to write more robust, cleaner OKRs with clear Objectives and authentic Key Results. It will make a difference!

### **North Star Metric and Metrics Tree**

Moving up in the concept novelty level, the training presented the concept of a North Star Metric (NSM).

While I heard about this tool several times and thought it made logical sense, I never saw it in use and couldn't figure out how to implement it for the strategic efforts I was part of.

The most mind-blowing moment was when the concept of a Metric Tree was introduced. Metric trees allow you to connect and align multiple teams/projects/products, giving each of them a fitting metric to focus on that connects and feeds into the NSM for the overarching organization (can even be the whole company).

![[Image](https://itamargilad.com/wp-content/uploads/2019/12/Metrics-tree-overlap-1024x435.jpg) from Itamar Gilad's site](https://miro.medium.com/0*IoTG2BBCRkVfmriy)

Spending time learning this enabled our Triad, myself, the product manager (PM) lead, the user experience (UX) lead, and the program manager (PgM) lead to implement it for the first time in our careers.

**=> Takeaway**
Thanks to the shared language, learning something together is critical to driving change. It's worth the risk of wasting people's time.

### **Outcome (not output) Based Roadmaps (OBRs)**

Those are roadmaps focused on problems/goals and metrics ([link ](https://itamargilad.com/outcome-roadmaps/)to article). So, instead of building "an ability for Googlers to book rooms," you will ideate ways and metrics to "Help Googlers find space for their meetings."

The first approach will lead you to build a room booking system. The 2nd approach can lead us to explore ways to free up rooms that were unlikely to be used, suggest public spaces for specific meetings and book meeting rooms for the rest.

Do you see the difference? The second approach opens many more options to help solve the problem. When we just focused on assisting employees in booking rooms, we failed to help Googlers, and we even amplified the problem sometimes. How? We made it so easy to book a room that people automatically booked one, often forgetting to release it when they did not need it (when working from home or redirecting to a hallway walk).

![[Image](https://itamargilad.com/outcome-roadmaps/) from Itamar Gilad's site](https://miro.medium.com/0*a69G6JFa20PbVvH9)

Outcome-based roadmaps use a goal and metrics with target numbers at the top. Under that, you can plug experiments, launches, ideas, and more. It's a way to avoid feature factory—launching software for the sake of launching rather than solving a user's pain or reaching a goal.

What's cool about it is that the features and actions under the goal and metric can change over time, which is just fine. OBRs offer a stable yet flexible structure for software organizations to focus and deliver on user pains and company goals.

**=> Takeaway**You heard it before: focusing on outcomes is better than on output. Now, you also have a practical way to make that change.

![[Image](https://itamargilad.com/wp-content/uploads/2024/01/AFTER-model-scaled.jpg) from Itamar Gilad's site](https://miro.medium.com/0*nGJcMGXfn1Ba2juH)

### **Wide Spectrum Validation**

Like any other tech leader, I was familiar with many validation methods, such as usability testing, user surveys and production a/b testing. I particularly liked the "fake button test" and "person behind the curtain." I know most people call it man behind the curtain, but I'd like to challenge the gender use here.

My team used several validation methods, but as I was parsing Itamar's Validation slide, it hit me that we hadn't used many more. What also excited me was the light bulb moment that validation is a funnel, going from left to right on the X axis while choosing which boxes of test types we'll be exercising.

The visual made it very clear to me that we're "leaving money on the table," and we have many more ways to test our solutions BEFORE we build a production version of the thing.

It was a pain I felt strongly and too frequently. Google engineers are so strong, production-focused, and eager to build top-notch tech that when presented with a problem, they spring to action and spit out high-quality production code. But... What if the solution is wrong? I don't want to wait 6+ months to figure that out, especially if we have spent the time making it production-level scalable code. That's wasteful and sad.

This was one of the visuals I frequently presented when discussing testing and building something. It helped me push our team to be lean and make less while we gain confidence in our solution.

**=> Takeaway**
Test first, build later. There are more ways to test than you've been using.

### **Impact Confidence Ease (ICE) Scoring**

**Impact**: How much impact can this feature, product, or idea drive for the company/effort?

**Confidence**: How confident are we in that impact? Otherwise, how much evidence have we collected to believe in this honestly? This is measured using the Confidence Meter—see the next section.

**Ease**: How easy is it to build/execute on this idea (usually measured in weeks of effort)?

What ICE brought to my mental model was two things:

First, it gave a simple and easy-to-follow framework for prioritizing and choosing from a set of options. Usually, ideas are used to test for either products or features. Other prioritization frameworks exist, but there wasn't a clear or agreed-upon framework in my 25 years of running software organizations. So, now I got one - huzzah!

Second, it reinforced the importance of testing multiple ideas in parallel. Why is this important? When I learned this, I realized I'd seen a repeating pattern. A project team will ideate to incept ideas (again, they can be features or products), pick up one of them, and give it a swirl. If they see non-negative results, i.e., some traction in the desired direction, they'd declare victory and often not bother to test the other ideas.

What if this idea was okay, but a perfect one was in the pile of skipped ideas?

Imagine you go fishing in the lake. You throw your line and wait. Finally, something pulls on it. You pull it out, and it's a fish! You declare victory and go home happy with your bounty.

![Photo by [stephen momot](https://unsplash.com/@ah360?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)](https://miro.medium.com/0*fhlaGPMuOB41pMtV)

But what if there are other fish in the lake? What if there is a bigger fish? A tastier fish? A more nutritious fish? You'll never know because you stopped your search after the first success.

When ICE takes a list of ideas through the validation matrix over a series of Steps, you get a measured race between several ideas. Every step increases the evidence you collect, and then you can choose the best one.

> "The vast **majority of [ideas] fail in experiments**, and even experts often misjudge which ones will pay off.

> At Google and Bing, **only about 10% to 20% of experiments generate _positive_ results**. "

> - Ron Kohavi and Stefan Thomke / Harvard Business Review

**=> Takeaway**To find the best solution, you must test multiple ideas in parallel. One idea won't give you what you need.

![[Image](https://itamargilad.com/wp-content/uploads/2018/10/be9a8-1cs2gqpb71moxqov7mw4yra.png) from Itamar Gilad's site](https://miro.medium.com/0*OWvk7SuiNg8bJnhH)

### **Confidence Meter**

Okay, this was a novel concept for me. Itamar presented a calculator called a confidence meter that solved my long-standing struggle.

That problem was pronounced during a demo session at the end of our first COVID-triggered hackathon. Each presenting team had a slide that said,

> "I need four engineers, one PM, one UX and nine months to build it."

It reflected how investment decisions were often made, at least in my neck of the woods. Anyone who says

> "This is how things are done at Google''

is an unintentional liar. Google is so big that we each have limited exposure and visibility to how things work. What they mean is

> "This is how my organization works."

My leadership chain made decisions based on product strategy decks and research. Suppose what was presented made sense to the decision makers (including their invisible biases) and was thematically aligned with the direction we were going in. In that case, they'd "bless" that project and invest headcount in it, standing up a team of X Googlers and letting them run for 1+ years. It was expected to make your first launch 12–18 months after the kick-off.

That meant... that it often took us 1.5 years to start seeing results if the solution was working or not.

That is an expensive approach if you ask me.

The confidence meter has a logarithmic gauge between 0 and 10. It represents the evidence we have when we make a build decision. Leadership opinion and thematic alignment (matching the strategic direction) were assigned 0.05. If you follow the confidence meter's approach, you should push towards real user data—getting vital feedback or actual usage from users—to reach a score of 3. To better understand this, go back to the visual above - and look at the numbers.

This was a game-changer! When decision-makers like or dislike something, we can pull up the confidence meter and align on the confidence level we can derive from our data. Then, we could focus on gaining more evidence to make a quality decision. This helped us avoid premature "start" and "stop" decisions. It was amazing.

**=> Takeaway**
Shift from opinions to data, using testing and validation methods.

## Key Lessons

- **Growth Mindset**: many experienced leaders get jaded. One such leader once told me: "Sivan, we don't need training. We have the knowledge and skills, we just need to focus" when I suggested we attend product training. I believe continuous learning significantly enhances both personal and team effectiveness. Learning something new or recalling something you stopped doing is never too late. E.g. OKRs.

- **Group Learning delivers more value:** yes, it takes time from many people, but it fosters a shared understanding and language and enables you to augment the development process together.

- **The Power of Visual Aids:** Incorporating visual tools into each concept and tool made it easy to extract value from them continuously. I used to pull them up to remind the team what we learned or explain the idea to a newcomer in a few sentences.
Visual tools and aids can significantly aid in understanding complex concepts and maintaining alignment on goals and strategies.

- **Evidence-Based Decision Making**: Emphasizing data and honest user feedback leads to more grounded and practical outcomes. Everyone talks about data evidence-based decisions, but only some follow those concepts.

- **Engineers Can Benefit from Product Consultants**: While it is expected to stick in our lane of technical execution, our jobs are the same as PMs (and UX and PgMs, etc.). We succeed when our work gets to the hands of users and creates the value the company needs to succeed—finding the better idea and jumping to build software when the right amount of confidence is reached benefits, everyone.

- **You Can Pick and Choose the tools your team is missing:** Even if you are convinced you mastered OKR writing, you can still acquire a new tool to improve or make your life easier.

---

Were any of the tools and concepts new to you? Which one?

Have you ever felt this way towards a framework you learned and advocate for? Do share!

---

Read the next story about launching products fast(er):

> [**Unlocking Agility at Scale: A Googler's Story**](https://code.likeagirl.io/unlocking-agility-at-scale-a-googlers-story-f0ea4c42d96c)

### Links for further reading:

- [GIST](https://itamargilad.com/gist-framework/) article

- [Outcome roadmaps](https://itamargilad.com/outcome-roadmaps/) ([free template](https://itamargilad.com/outcomes-roadmap-template/))

- [North Star Metric and Metric Trees](https://itamargilad.com/the-three-true-north-metrics-that-your-product-and-business-need/)

- [ICE Scoring](https://itamargilad.com/ice-scores/)

- [The Confidence Meter ](https://itamargilad.com/the-tool-that-will-help-you-choose-better-product-ideas/)([free download](https://itamargilad.com/resources/confidence-meter-calculator/))

- [Idea validation (The AFTER Framework)](https://itamargilad.com/idea-validation-much-more-than-just-a-b-experiments/)

- [The GIST Board](https://itamargilad.com/the-gist-board-and-other-gist-tools/) ([Template](https://itamargilad.com/resources/gist-board-template/))