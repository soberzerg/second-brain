---
tags:
  - youtube
  - выжимка
  - ai
  - security
  - ai-agents
  - cybersecurity
source: "https://youtu.be/ceEUO_i7aW4?si=BJlRFy-Ww9xsd_tB"
date: 2026-02-10
video_title: "Personal AI Agents Like OpenClaw Are a Security Nightmare"
---

# Выжимка: Personal AI Agents Like OpenClaw Are a Security Nightmare

**Источник:** https://youtu.be/ceEUO_i7aW4?si=BJlRFy-Ww9xsd_tB
**Дата:** 2026-02-10

## Основная тема

Видео анализирует серьезные уязвимости безопасности в экосистеме OpenClaw (включая Clawbot, Moldbot и платформу ClawHub), указывая на обнаружение "спящих агентов", массовую утечку API-ключей и распространение вредоносного ПО через сторонние навыки. Персональные ИИ-агенты становятся критически опасными из-за их способности понимать и выполнять семантические инструкции.

## Ключевые тезисы

1. **"Спящие агенты" (Sleeper Agents)** - Исследователи Cisco обнаружили случаи установки "спящих агентов" на компьютеры пользователей OpenClaw, которые не активируются сразу, а ожидают определенного "секретного кодового слова" в течение дней, недель или месяцев. Это делает угрозу скрытой и труднообнаруживаемой.

2. **Компрометация операционной системы** - Злоумышленники обучили ботов "выходить из безопасных Docker-контейнеров" (изолированных сред) и устанавливать вредоносное ПО непосредственно в реальную систему пользователя, минуя защитные механизмы.

3. **Массовая утечка конфиденциальных данных** - Произошла утечка 1.5 миллиона API-ключей (OpenAI, Anthropic, AWS, Gemini и др.), 35 000 электронных адресов пользователей и более 4000 личных сообщений между ИИ-агентами из-за недостаточного шифрования и хранения в чат-логах.

4. **Распространение вредоносного ПО через навыки (Skills)** - Многие из самых популярных навыков, доступных на ClawHub, были заражены вредоносным ПО. Например, популярный навык "What Would Elon Do" был скомпрометирован и распространял малварь.

5. **Семантические уязвимости (Semantic Vulnerabilities)** - С появлением больших языковых моделей (LLM), ИИ-агенты теперь способны "понимать", что написано в текстовых файлах (.txt, .md). Это означает, что вредоносные команды могут быть встроены в обычные инструкции, и агент, стремясь быть "полезным", будет их выполнять.

6. **Эксплуатация через установку зависимостей (Prerequisites)** - Вредоносный навык может содержать инструкцию для агента по установке "необходимых условий" (prerequisites), которая на самом деле ведет к загрузке и выполнению обфусцированной вредоносной программы. Эта программа может отключать встроенные защитные механизмы ОС, например, Gatekeeper в macOS.

7. **Сохранность чат-логов как уязвимость** - Чат-логи агентов, даже если они не зашифрованы, могут содержать конфиденциальные API-ключи, переданные агенту. Эти логи сохраняются и представляют собой постоянный риск утечки данных, даже если агент сам корректно хранит ключи в файлах `.env`.

8. **Баланс между возможностями и безопасностью** - Высокая функциональность ИИ-агентов в OpenClaw достигается за счет ослабления мер безопасности (guard rails and security are turned off or really turned down), что делает их мощными, но и чрезвычайно опасными.

## Полезные инсайты

> Неочевидные выводы и интересные наблюдения из видео

- **Новая природа угроз** - Эпоха "дикого запада" в сфере ИИ-агентов означает, что угрозы переходят от технических эксплойтов багов к семантическим атакам, где сам язык становится вектором для внедрения вредоносных команд.

- **ИИ против ИИ** - Развитие ИИ-инструментов для атаки стимулирует создание ИИ-инструментов для защиты. Cisco Skill Scanner, основанный на LLM, демонстрирует, как ИИ может использоваться для выявления семантических угроз в навыках других ИИ-агентов.

- **Цена удобства** - Упрощение процесса настройки ИИ-агентов и их интеграции с внешними сервисами (например, через чат-окна) часто ведет к компромиссам в безопасности, о которых пользователи могут не подозревать.

- **Риски тестирования** - Энтузиасты, которые "идут напролом" и тестируют новые ИИ-технологии с минимальными ограничениями, играют роль "подопытных кроликов", помогая выявлять уязвимости и ускорять прогресс в области безопасности, хотя сами при этом несут значительные риски.

## Практические рекомендации

> Что можно применить на практике

- [ ] **Немедленная смена API-ключей** - Если вы использовали OpenClaw или загружали навыки с ClawHub, немедленно смените все свои API-ключи для всех подключенных сервисов (OpenAI, Anthropic, AWS, Gemini и др.).

- [ ] **Переустановка системы или полный сброс** - Рассмотрите возможность полной переустановки операционной системы, чтобы гарантировать удаление всех возможных "спящих агентов" и вредоносного ПО.

- [ ] **Ручное управление API-ключами** - Вносите API-ключи непосредственно в файлы `.env` или аналогичные безопасные хранилища переменных окружения, НИКОГДА не передавайте их агенту через чат-интерфейс.

- [ ] **Создание собственных навыков** - Разрабатывайте собственные навыки (skills) с нуля, чтобы иметь полный контроль над тем, что ваш ИИ-агент может делать и к каким ресурсам имеет доступ.

- [ ] **Использование Skill Scanner** - Для проверки навыков, скачанных из сторонних источников, используйте инструменты, такие как Cisco Skill Scanner (доступен на GitHub под организацией Cisco AI Defense), который анализирует код навыка на предмет подозрительных команд и поведения.

- [ ] **Ограничение доступа к ресурсам** - Настраивайте минимально необходимые разрешения для каждого API-ключа и каждой интеграции, чтобы минимизировать потенциальный ущерб в случае компрометации.

- [ ] **Очистка чат-логов** - Регулярно удаляйте или очищайте чат-логи агентов, особенно если в них когда-либо хранилась конфиденциальная информация.

## Упомянутые ресурсы

> Книги, инструменты, ссылки, эксперты

- **OpenClaw** (платформа AI-агентов) - Основная платформа, в которой обнаружены уязвимости
- **Clawbot** (AI-агент) - Один из агентов экосистемы OpenClaw
- **Moldbot** (AI-агент) - Другой агент экосистемы с утечкой данных
- **ClawHub** (маркетплейс навыков) - Платформа для распространения навыков агентов, многие из которых заражены
- **Cisco Skill Scanner** (инструмент безопасности) - LLM-based инструмент для анализа навыков на предмет вредоносного кода, доступен на GitHub (Cisco AI Defense)
- **Daniel Locklear** (исследователь) - Исследователь из Cisco, работающий над проблемами безопасности AI-агентов
- **Amy Chang** (исследователь) - Лидер по исследованиям угроз и безопасности ИИ в Cisco
- **Vineet Sai** (исследователь) - Соавтор отчета Cisco о уязвимостях OpenClaw
- **Wiz** (компания) - Исследователи обнаружили утечку данных Moldbot
- **Docker** (технология) - Контейнеризация, которую злоумышленники научились обходить
- **Gatekeeper** (защита macOS) - Встроенная защита macOS, которую отключает вредоносное ПО

## Связанные темы

> Ссылки на другие заметки в vault

- [[01_Projects/AGIents.pro - ИИ-агенты платформа/README.md]] - критически важно учесть уроки безопасности при разработке платформы агентов
- [[01_Projects/TG Claude Code Assistant/masterplan.md]] - применить best practices безопасности для API-ключей и навыков
- [[03_Resources/ИИ-технологии/Boris Cherny - Claude Code Development Workflow.md]] - сравнить подход к безопасности в Claude Code vs OpenClaw

## Заметки

> Личные мысли и выводы

**Критические уроки для AGIents.pro:**

1. **Безопасность навыков (Skills)** - Нужна система верификации и sandboxing для сторонних навыков. Возможно, создать whitelist проверенных навыков или собственный маркетплейс с модерацией.

2. **Управление API-ключами** - Абсолютно исключить возможность передачи API-ключей через чат. Только прямая конфигурация в `.env` или защищенное хранилище секретов.

3. **Логирование и приватность** - Чат-логи НЕ должны содержать конфиденциальную информацию. Возможно, использовать маскирование чувствительных данных или шифрование.

4. **Docker изоляция** - Недостаточно полагаться только на Docker-контейнеры. Нужны дополнительные слои защиты (network policies, limited syscalls).

5. **Semantic Security** - Новый вектор атак через естественный язык. Нужны guard rails на уровне промптов и возможность отклонять подозрительные команды.

**Для TG Claude Code Assistant:**
- Не допускать установку сторонних skills без ручной проверки
- API-ключи только через environment variables, никогда через Telegram
- Логи чатов с маскированием чувствительной информации
- Whitelist команд вместо blacklist

**Возможная идея для поста (AISobolev):**
Разбор кейса OpenClaw как пример "что НЕ надо делать" при создании AI-агентов платформы. Контраст с подходом Claude Code (безопасность by design).

---

*Выжимка создана автоматически с помощью youtube-summary skill*
*Модель: Gemini Vision*
