### Typical RAG Process, Best Practices for Each Module, and Comprehensive Evaluation

The process of RAG is complex, with numerous components. How can we determine the existing RAG methods and their optimal combinations to identify the best RAG practices?

This article introduces a new study titled "[Searching for Best Practices in Retrieval-Augmented Generation](https://arxiv.org/pdf/2407.01219)". This study aimes to address this problem.

This article is divided into four main parts. First, it introduces the typical RAG process. Next, it presents best practices for each RAG module. Then, it provides a comprehensive evaluation. Finally, it shares my thoughts and insights, and concludes with a summary.

## Typical RAG Workflow

![Figure 1: Retrieval-augmented generation workflow[.](https://arxiv.org/pdf/2407.01219.) The optional methods considered for each component are indicated in bold fonts, while the methods underlined indicate the default choice for individual modules. The methods indicated in blue font denote the best-performing selections identified empirically. Source: [Searching for Best Practices in Retrieval-Augmented Generation](https://arxiv.org/pdf/2407.01219).](https://miro.medium.com/0*p_LnCvVGjvtrriKF.png)

A typical RAG workflow includes several intermediate processing steps:

- [Query classification](https://ai.gopubby.com/advanced-rag-11-query-classification-and-refinement-2aec79f4140b) (determining if the input query requires retrieval)

- Retrieval (efficiently obtaining relevant documents)

- [Re-ranking](https://pub.towardsai.net/advanced-rag-04-re-ranking-85f6ae8170b1) (optimizing the order of retrieved documents based on relevance)

- Re-packing (organizing the retrieved documents into a structured form)

- [Summarization](https://ai.gopubby.com/advanced-rag-09-prompt-compression-95a589f7b554) (extracting key information to generate responses and eliminate redundancy)

Implementing RAG also involves deciding how to split documents into chunks, choosing which embeddings to use for semantic representation, selecting a suitable vector database for efficient feature storage, and finding effective methods for fine-tuning LLMs, as shown in Figure 1.

## Best Practices of Each Steps

### Query Classification

Why is [query classification](https://ai.gopubby.com/advanced-rag-11-query-classification-and-refinement-2aec79f4140b) needed? Not all queries require retrieval enhancement, as LLMs have certain capabilities. While RAG can improve accuracy and reduce hallucinations, frequent retrieval increases response time. Therefore, we classify queries first to determine if retrieval is needed. Generally, retrieval is recommended when knowledge beyond the model's parameters is required.

We can classify tasks into 15 types based on whether they provide sufficient information and display specific tasks and examples. Tasks based entirely on user-provided information are marked as "sufficient" and do not require retrieval; otherwise, they are marked as "insufficient" and may require retrieval.

![Figure 2: Classification of retrieval requirements for different tasks[.](https://arxiv.org/pdf/2407.01219.) In cases where information is not provided, we differentiate tasks based on the functions of the model. Source: [Searching for Best Practices in Retrieval-Augmented Generation](https://arxiv.org/pdf/2407.01219).](https://miro.medium.com/0*StjuOeau7XjTJ0uF.png)

This classification process is automated by training a classifier.

![Figure 3: Results of the Query Classifier[.](https://arxiv.org/pdf/2407.01219.) Source: [Searching for Best Practices in Retrieval-Augmented Generation](https://arxiv.org/pdf/2407.01219).](https://miro.medium.com/1*ILnFJng9ecudvsvV82-YCg.png)

### Chunking

Dividing the document into smaller chunks is crucial for improving retrieval accuracy and avoiding length issues in LLM. There are generally three levels:

- Token-level chunking is straightforward but may split sentences, affecting retrieval quality.

- [Semantic-level chunking](https://pub.towardsai.net/advanced-rag-05-exploring-semantic-chunking-97c12af20a4d) uses LLM to determine breakpoints, preserving context but taking more time.

- Sentence-level chunking balances preserving text semantics with being concise and efficient.

Here, sentence-level chunking is used to balance simplicity and semantic retention. The chunking process is evaluated from the four dimensions below.

**Chunk Size**

Chunk size significantly affects performance. Larger chunks provide more context, enhancing comprehension, but increase processing time. Smaller chunks improve recall rates and reduce time but may lack sufficient context.

![Figure 4: Comparison of different chunk sizes[.](https://arxiv.org/pdf/2407.01219.) Source: [Searching for Best Practices in Retrieval-Augmented Generation](https://arxiv.org/pdf/2407.01219).](https://miro.medium.com/1*frCwTr7EBOrQ64jO38KR2w.jpeg)

As shown in Figure 4, two main metrics are used: faithfulness and relevancy. Faithfulness measures whether the response is hallucinatory or matches the retrieved text. Relevancy measures whether the retrieved text and the response match the query.

**Organization of Chunks**

The results are shown in Figure 5. The smaller chunk size is 175 tokens, the larger chunk size is 512 tokens, and the block overlap is 20 tokens.

![Figure 5: Comparison of different chunk skills[.](https://arxiv.org/pdf/2407.01219.) Source: [Searching for Best Practices in Retrieval-Augmented Generation](https://arxiv.org/pdf/2407.01219).](https://miro.medium.com/0*hDOWEBKbYX_CTxlc.png)

**Embedding Model Selection**

As shown in Figure 6, [LLM-Embedder](https://arxiv.org/pdf/2310.07554) achieved results comparable to BAAI/bge-large-en but is only one-third its size. Therefore, LLM-Embedder is chosen to balance performance and size.

![Figure 6: Results for different embedding models on namespace-Pt/msmarco[.](https://arxiv.org/pdf/2407.01219.) Source: [Searching for Best Practices in Retrieval-Augmented Generation](https://arxiv.org/pdf/2407.01219).](https://miro.medium.com/0*3SPRLMhpPqv5tvR9.png)

**Metadata Addition**

Enhancing chunk blocks with metadata like titles, keywords, and hypothetical questions can improve retrieval.

The paper does not include specific experiments but leaves them for future work.

### Vector Databases

Figure 7 provides a detailed comparison of five open-source vector databases: Weaviate, Faiss, Chroma, Qdrant, and Milvus.

Milvus stands out among the evaluated databases, meeting all the basic criteria and outperforming other open-source options in performance.

![Figure 7: Comparison of Various Vector Databases[.](https://arxiv.org/pdf/2407.01219.) Source: [Searching for Best Practices in Retrieval-Augmented Generation](https://arxiv.org/pdf/2407.01219).](https://miro.medium.com/0*OCNhUa8e5-vsDjOx.png)

### Retrieval

For user queries, the retrieval module selects the top k documents most relevant to the query from a pre-constructed corpus, based on their similarity.

The following evaluates three retrieval-related techniques and their combinations:

- [Query Rewriting](https://medium.com/@florian_algo/advanced-rag-06-exploring-query-rewriting-23997297f2d1): This technique improves queries to better match relevant documents. Inspired by the [Rewrite-Retrieve-Read](https://medium.com/@florian_algo/advanced-rag-06-exploring-query-rewriting-23997297f2d1) framework, we prompt LLM to rewrite queries to enhance performance.

- Query Decomposition: This method retrieves documents based on sub-questions extracted from the original query. These sub-questions are typically more complex and difficult to understand and process.

- [Pseudo-Document Generation](https://medium.com/@florian_algo/advanced-rag-06-exploring-query-rewriting-23997297f2d1): This method generates a hypothetical document based on the user's query and uses the embeddings of the hypothetical answer to retrieve similar documents. A notable implementation is [HyDE](https://medium.com/@florian_algo/advanced-rag-06-exploring-query-rewriting-23997297f2d1).

![Figure 8: Results for different retrieval methods on TREC DL19/20[.](https://arxiv.org/pdf/2407.01219.) The best result for each method is made bold and the second is underlined. Source: [Searching for Best Practices in Retrieval-Augmented Generation](https://arxiv.org/pdf/2407.01219).](https://miro.medium.com/0*F3qneBPqm6PFnIsZ.png)

Figure 8 shows that the supervised methods significantly outperform the unsupervised methods. By combining HyDE and hybrid search, [LLM-Embedder](https://arxiv.org/pdf/2310.07554) achieved the highest score.

Therefore, it is recommended to use HyDE + hybrid search as the default retrieval method. Hybrid search combines sparse retrieval (BM25) and dense retrieval (original embeddings), achieving high performance with relatively low latency.

### Re-ranking

After an initial search, a [re-ranking phase](https://pub.towardsai.net/advanced-rag-04-re-ranking-85f6ae8170b1) enhances the relevance of the retrieved documents, ensuring the most pertinent information appears at the top of the list. Two main methods are considered:

- DLM Re-ranking: This method uses Deep Language Models (DLMs) for re-ranking. These models are fine-tuned to classify the relevance of documents to queries as "true" or "false." During fine-tuning, the models are trained with queries and documents annotated for relevance. During inference, documents are sorted based on the probability of the "true" label.

- TILDE Re-ranking: TILDE independently calculates the likelihood of each query term by predicting the probability of each term in the model's vocabulary. Documents are scored by summing the precomputed log probabilities of the query terms, enabling fast re-ranking during inference. TILDEv2 improves this by only indexing terms present in the documents, using NCE loss, and expanding documents, thereby increasing efficiency and reducing index size.

![Figure 9: Results of different reranking methods on the dev set of the MS MARCO Passage ranking dataset[.](https://arxiv.org/pdf/2407.01219.) For each query, the top-1000 candidate passages retrieved by BM25 are reranked. Latency is measured in seconds per query. Source: [Searching for Best Practices in Retrieval-Augmented Generation](https://arxiv.org/pdf/2407.01219).](https://miro.medium.com/0*6Avn6NugtHx41k2l.png)

As shown in Figure 9, **it is recommended to use [monoT5](https://aclanthology.org/2020.findings-emnlp.63.pdf)** as a comprehensive method balancing performance and efficiency. [RankLLaMA](https://arxiv.org/pdf/2310.08319) is ideal for those seeking optimal performance, while TILDEv2 is suitable for quickly experimenting on a fixed set.

### Re-packing

The performance of subsequent processes, such as LLM response generation, may be affected by the order in which documents are provided.

To address this, we have included a compact re-packing module in the workflow after re-ranking, with three methods:

- The "forward" method repacks documents in descending order based on relevance scores from the reordering phase.

- The "reverse" method arranges them in ascending order.

- The "sides" option, inspired by [Lost in the Middle](https://arxiv.org/pdf/2307.03172), performs best when relevant information is located at the beginning or end of the input.

Since these re-packing methods mainly affect subsequent modules, their evaluation is introduced in the following comprehensive review section.

### Summarization

Retrieval results may contain redundant or unnecessary information, which can prevent the LLM from generating accurate responses. Additionally, long prompts may slow down the inference process. Therefore, effective methods to summarize retrieved documents are crucial in the RAG process.

Extractive compressors segment the text into sentences, scoring and ranking them based on importance. Generative compressors synthesize information from multiple documents to rephrase and generate coherent summaries. These tasks can be either query-based or non-query-based.

Three methods are primarily evaluated:

- [Recomp](https://ai.gopubby.com/advanced-rag-09-prompt-compression-95a589f7b554): It features both extractive and generative compressors. The extractive compressor selects useful sentences, while the generative compressor synthesizes information from multiple documents.

- [LongLLMLingua](https://ai.gopubby.com/advanced-rag-09-prompt-compression-95a589f7b554): It improves LLMLingua by focusing on key information relevant to the query.

- [Selective Context](https://ai.gopubby.com/advanced-rag-09-prompt-compression-95a589f7b554): It improves LLM efficiency by identifying and removing redundant information in the input context.

![Figure 10: Comparison between different summarization methods[.](https://arxiv.org/pdf/2407.01219.) Source: [Searching for Best Practices in Retrieval-Augmented Generation](https://arxiv.org/pdf/2407.01219).](https://miro.medium.com/0*rHY7S0H4rmoonX_5.png)

As shown in Figure 10, **it is recommended to use** **Recomp** because it performs excellently. Although LongLLMLingua did not perform well, it demonstrated better generalization ability without being trained on these experimental datasets. Therefore, we can consider it an alternative method.

### Generator Fine-tuning

Figure 11 shows that the model trained with mixed relevant and random documents (Mgr) performs best when provided with gold documents or mixed context.

Therefore, **mixing relevant and random context during training can enhance the generator's robustness to irrelevant information while ensuring effective use of relevant context.**

![Figure 11: Results of generator fine-tuning[.](https://arxiv.org/pdf/2407.01219.) Source: [Searching for Best Practices in Retrieval-Augmented Generation](https://arxiv.org/pdf/2407.01219).](https://miro.medium.com/0*WEw2_hZAkqGIgOcZ.png)

## Comprehensive Evaluation

Previous evaluations were conducted separately for each module, but now these modules are integrated for a comprehensive evaluation.

![Figure 12: Results of the search for optimal RAG practices[.](https://arxiv.org/pdf/2407.01219.) Modules enclosed in a boxed module are under investigation to determine the best method. The underlined method represents the selected implementation. The "Avg" (average score) is calculated based on the Acc, EM, and RAG scores for all tasks, while the average latency is measured in seconds per query. The best scores are highlighted in bold. Source: [Searching for Best Practices in Retrieval-Augmented Generation](https://arxiv.org/pdf/2407.01219).](https://miro.medium.com/0*fg2MfS7AwzMNAcMq.png)

As shown in Figure 12, the following key insights are derived:

- **Query Classification Module:** This module not only improves effectiveness and efficiency but also increases the overall score from 0.428 to an average of 0.443, and reduces query latency from 16.41 seconds to 11.58 seconds.

- **Retrieval Module:** While the "Hybrid with HyDE" method achieved the highest RAG score of 0.58, **its computational cost is high**, requiring 11.71 seconds per query. **Therefore, it is recommended to use the "Hybrid" or "Original"** methods as they reduce latency while maintaining comparable performance.

- **Re-ranking Module:** The absence of the re-ranking module results in a significant drop in performance. MonoT5 achieved the highest average score, demonstrating its effectiveness in enhancing the relevance of retrieved documents. This indicates the crucial role of re-ranking in improving the quality of generated responses.

- **Re-packing Module:** The reverse configuration exhibited superior performance, achieving a RAG score of 0.560. This suggests that placing more relevant context closer to the query position yields the best results.

- **Summarization Module:** Recomp demonstrated superior performance, although removing the summary module can achieve comparable results at lower latency. Nevertheless, Recomp remains the preferred choice because it addresses the generator's maximum length limitation.

## My Thoughts and Insights

From this paper, I gleaned several insights:

- **Importance of system components**: The paper highlights the significance of each component in the RAG system - such as query classification, retrieval, reranking, document repacking, summarization, and generation. It demonstrates that optimizing the performance of individual components is crucial when designing complex systems.

- **Importance of modular design**: Optimizing and testing components separately show the benefits of modular design in complex systems. It enhances maintainability by allowing updates and optimizations independently, facilitating reuse and adjustment across various applications.

- **Systematic experimental methodology**: By conducting extensive tests across well-recognized datasets, the paper ensures the reliability and generalizability of its results. This systematic approach to experimental design serves as an excellent example for other researchers.

There are also some challenges in practical application of RAG

- **Generalization**: We should note that the evaluation mentioned above is mainly based on public mainstream datasets, and its performance on other datasets (such as enterprise private datasets) needs further assessment.

- **Coverage**: For instance, it did not cover late interaction models like ColBERT for re-ranking, nor did it evaluate techniques like [Graph RAG or RAPTOR that enhance global understanding](https://medium.com/ai-advances/advanced-rag-12-enhancing-global-understanding-b13dc9a8db39). Looking forward to future updates.

- **Real-time performance**: Although the paper considers the speed of retrieval and response, balancing speed with accuracy in real-time applications remains a challenge.

- **Integration and processing of multi-modal data**: Effectively processing and integrating data from different modalities to ensure system stability and effectiveness is another technical hurdle that needs addressing.

## Conclusion

Overall, two different RAG system implementation strategies are recommended:

- Best Performance Practice: For the highest performance, include a query classification module, use the "Hybrid with HyDE" method for retrieval, adopt monoT5 for re-ranking, choose "Reverse" for re-packing, and utilize Recomp for summarization.

- Balanced Efficiency Practice: To balance performance and efficiency, include a query classification module, implement the Hybrid method for retrieval, use TILDEv2 for reranking, choose "Reverse" for re-packing, and use Recomp for summarization.

**The main value of this paper is that it offers valuable ideas and methods for studying RAG best practices.**

If you're interested in RAG technologies, feel free to check out my other articles.

> [**RAG**](https://medium.com/@florian_algo/list/1b65363c06db)

**And the latest article or video can be found in [my newsletter](https://florianjune.substack.com/) or on [YouTube](https://www.youtube.com/@ai_exploration_journey)**.

Finally, if there are any errors or omissions in this article, or if you have any questions, please point them out in the comment section.