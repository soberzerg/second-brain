
![The growth of early NLP methods to the modern transformer model (Image generated by Bing Copilot)](https://miro.medium.com/1*XRNv7_5PuMu2uQSy8IAd0w.jpeg)

## A brief preface...

As usual, I offer a brief preface to illuminate some context and motivations for writing this. I'm a deeply curious undergraduate Applied Mathematics student at Columbia University who loves CS, LLMs, mathematics, and any related theoretical underpinnings.

LLMs rely on vector embeddings, and in this article we will break down vector embeddings of words, in particular, and how they are used for models to understand natural language. It is easy to throw around the term "vector embeddings" without understanding the technical details. I hope this post will help you understand embeddings' motivations and ingenuity by offering an intuitive balance of technical and high-level detail.

## Roadmap

Below is a roadmap of everything that I will cover in this post. Do not fret about strange terminologies if you have little CS or math background. My segues between sections will be intuitive and smoother than peanut butter at room temperature. Part II Next Level will be fairly abstract and high-level, and I'll likely write some subsequent mathematics-heavy posts to offer the same walk-through intuition that I provide in Part I Basics.

### **Motivations: Why Natural Language Processing? Why vector embeddings?**

### **I. Basics**

**One-Hot Encoding**

**Bag of Words (BOW)**

**TF-IDF**

### **II. Next Level**

**Continuous Bag of Words (CBOW)**

- **Neural Networks Motivation**

- **High-Level CBOW NN Process**

**Skip-Gram**

**Transformer**

## Motivations: Why NLP? Why vector embeddings?

As we live our lives casually playing around with LLMs like ChatGPT, etc. it's quite easy to forget how wild it is that computers can now understand text pretty darned well. It has not always been this way. Preliminary research in this field started back in the 1990s, and the dream for conversational capabilities in machines has been around for even longer (not to mention that this idea has been in science fiction for a _really_ long time). NLP, or natural language processing, is the field that focuses on computers' ability to understand human dialogue. The field involves statistics, probability, lots of mathematics in general, and computer science. The high-level objective for NLP experts is to improve the way that computers understand natural, human language. Not surprisingly, researchers sought to find ways to accomplish this. Let's fast-forward and intuit that since computers deal with numbers well, we should try to use numbers and math to bridge the gap between natural language and computers.

## I. Basics

### One-Hot Encoding

> Motivating Question: How should we represent words as numbers, given a series of words in a piece of text?

For now, let's assume that our entire text is a simple sentence, "The color of nature's sky is blue." I know this sentence sounds weird, but just bear with me; it helps me make my points later on. In reality, we might have vast quantities of text to deal with. How can we represent this sentence in a computer model-readable format?

At first, we try to represent each word with a simple integer. So the=0, color=1, ..., blue=5. This looks good! Now we have every word labeled so the computer can deal with this representation of the text.

![](https://miro.medium.com/1*ZGqJQkJqNDV2EkJx3hBE-Q.png)

However, consider what happens when we compare the words via basic mathematical operations. Why does "color"+"nature"="sky" (because 1+3=4), and why is "of" squared equivalent to "sky" (because 2 squared = 4)? What does this even mean? Do these accurately represent the words (i.e. their grammatical essence, positioning, and relative importance)? This quickly becomes a problem when we use _vast_ sequences of text because the value of the last word will be quite large, and the computer will likely make the false interpretation that this word is the most important.

> Motivating Question: How can we represent (encode) words to avoid the problem above?

What are other ways that computers understand numbers? 0s and 1s, of course! Our sentence is 7 words long, so let's create 7 different vectors of all zeroes. For each word, we'll change one of the zero values to a one to represent where the word is in the sequence. Below are examples of two of the seven binary vectors.

![](https://miro.medium.com/1*08Ndx9XynPNiDOHH0sJ4Lw.png)

Thus, we now have a very rudimentary way to represent words' positions without insinuating odd, inaccurate relationships between them.

## Bag of Words (BOW)

> Motivating Question: One-hot encoding provides positional information. How can we start to include information about words' importance?

Now that we have locked in some simple positional information, we start wondering how to include relationships between words. Before we go ahead, let's add two more sentences so our _document_ (our text example):

![](https://miro.medium.com/1*sTBkBeuWN4WWmQtVUzyoZg.png)

When a human reads this, it is clear that color is a pretty significant idea. Not coincidentally, "color" appears more often than any other word (except the filler words "the," "of," and "is"); in other words, it has a higher frequency than the other words. Thus, maybe we should try to include this "frequency of each word" in our representation to gain some baseline information about word importance. Dog-ear this in your head for a second.

> Remark: As we'll see, the words with the highest frequencies are actually _not_ the ones we want to emphasize. Think of filler words!

We have another problem with our original binary (0s and 1s) representation for each word! When we get vast documents with multitudes of tokens, the length (dimension) of each vector _and_ **** the number of vectors increase. This is a huge problem (literally) because as we approach larger and larger document sizes, we will need more and more computational resources and time for any complex computations. We need to avoid this so that we can efficiently handle documents of any size. This problem is also known as the **curse of dimensionalit**y.

![](https://miro.medium.com/1*0-7H4fDnnkkj3-FodbslZQ.png)

> Motivating Question: How can we include word frequency information while avoiding the curse of dimensionality?

We come back to our frequency of words idea from above. Let's create one vector that includes the frequencies of each word in the sequence. So from our three-sentence document, our vector will have frequency information for each distinct token. Again, we see that "color," "the," "is," and "of" appear the most frequently in our document, and this sets us down the road to find ways to represent the relevant importance of words in our document. For now, let's simply create a vector that has the all the frequencies of each token (i.e. the **TF, or term frequency**, of each token).

> Remark: You may wonder what to do with repetitive terms. We will treat each word/token as its own entity, regardless of redundancy with other identical tokens. So both the first and second "color" tokens within our document will have TF = 1/7 = ~0.143 = ~14.3%

![](https://miro.medium.com/1*iUgJt9nSwCxCHY5OIpKymw.png)

There are variations of BOW such as Bag-of-Phrases and Bag-of-Ngrams. Instead of considering distinct words, these methods might identify frequencies of slightly larger sizes. In our example document, for instance, the Bag-of-Phrases or Bag-of-Ngrams methods might find the frequency of "color of the". They use the same underlying concepts.

## TF-IDF (Term Frequency-Inverse Document Frequency)

> Motivating Question: BOW counts frequencies of words in documents but doesn't consider significance or relationships between words. Can we do better than BOW?

Let's add two more documents in addition to our example document from earlier. They cover similar topics but they differentiate from each other in particular ways. This _collection of documents_ is called a **corpus**. Consider the following _three_ documents that discuss color, but talk about color in different contexts:

![Doc 1, Doc 2, Doc 3 in order from top to bottom](https://miro.medium.com/1*W9pgWg_DS3QyeFmLgAUDcg.png)

Previously, we identified that the frequency of "color" was useful information in a document. We'd like to continue to use the **term frequency (TF)** of each word in each document. As seen before, Doc 1's first "color" token at position i = 1 has TF = 3/21 = 1/7 = ~0.143 = 14.3% (as shown in the last BOW illustration above).

> Motivating Question: With multiple documents, how do we (1) identify the most important words within each document, and (2) differentiate documents from each other?

Our second motivating question suggests that we find a way to differentiate documents from each other. Note that our documents do indeed cover three different general topics: nature, food, and desk items. I did this intentionally for demonstration purposes. Distinguishing the documents based on these different topics although all three discuss color would be wonderful so we can extract more important information.

Previously, we noted that filler words such as "the" or "of" are most common in text. In any given text, filler words probably take up a sizable percentage of the total words. Therefore, we want higher frequency words that are also common across all our documents to have less _weight_ because they surely won't help distinguish our documents. So it then follows that rarer words (hypothetically, words like "nature," "foods," or "accessories" in our simple example) should be given more weight.

First, we want to find how rare a particular term is. The first way to do this that comes to mind is to create a document frequency for a given term similar to our TF (i.e. we want to find what fraction of our documents contain the particular term). For now, don't worry about what IDF actually stands for and let it stand for this document frequency (it stands for inverse document frequency, but hold your horses). Let's try the following:

![](https://miro.medium.com/1*hUSpKBBEayHYUTzKx34Twg.png)

But this doesn't doesn't work for our purposes because for the common filler words that are more common, the numerator is greater because, naturally, more documents will have filler words. This means that our IDF value will be higher for filler words; this is the exact opposite of what we want. How can we change this formula so the IDF value _decreases_ as the number of documents containing a term increases? Let's try to flip the formula (this is where "inverse" comes from):

![](https://miro.medium.com/1*bW5T7R9zHYsqlJe6CmGUnA.png)

Awesome. As before, the total # of docs is constant; as we increase the number of docs containing a term, the IDF value gets _smaller_. Thus, rarer words get larger IDF values. Perfect.

We have one last consideration for the IDF value. For a large corpus, IDF values can get extremely large or extremely small. Without going too much into the mathematical intuition, applying a logarithm to our entire fraction scales our IDF values down, making them more interpretable.

![](https://miro.medium.com/1*qBiGRbUN6cl4ROwOENb5rQ.png)

Finally, we have a TF value that represents local document frequency, and an IDF value that shows the global importance of a term across the entire set of documents (corpus). How can we combine these to yield a useful metric? The simplest way to do this is to multiply them!

![](https://miro.medium.com/1*PVNekfYkuw8UU0GzkMt1Bg.png)

Consider a token has a high TF value in a particular document and a high IDF value (i.e. it often shows up elsewhere in the corpus' other documents). Now consider a token that has the same TF in a particular document and __ a very low IDF. The latter token is probably a filler word and not useful to derive any sort of meaning. Thus, the TF-IDF value of the former word will be higher and yield more importance.

The way we represent all tokens' TF-IDF value is through a **TF-IDF matrix** where rows represent documents and columns represent words in the vocabulary (see below image).

![](https://miro.medium.com/1*bJD2DWDbMUs1RWMBEf_BjQ.png)

## II. Next Level

## Continuous Bag-Of-Words (CBOW)

> Motivating Question: TF-IDF utilizes a frequency-based approach to find the most important words. How can we capture syntactic and semantic meaning of a token based on surrounding context?

The primary goal of CBOW is to predict a target word based on its surrounding context. Consider this sentence: "Jason Statham is an _______." Ideally, the model ought to accurately guess "actor" here. CBOW uses a **neural network (NN)** architecture to help us with this prediction. Thus, the next couple of subsections will offer the necessary NN basics that power CBOW. I will try to keep the explanations pretty high level and save an intuitive deep dive of the underlying mathematics for another blog.

### **Neural Networks Motivation**

Consider the way you read "Jason Statham is an _____." You may automatically fill in the blank with "amazingly ripped dude" or "actor". Perhaps you just think of "man" because you've never watched Fast and Furious. Regardless, we know that a word like "octopus" is absolutely incorrect because over time and experience, our minds have been trained to make solid predictions in this sort of context. From experience, we know that Jason Statham is not, in fact, an octopus, although admittedly that would be pretty intriguing. Neural networks attempt to incrementally develop this experience to make human-like guesses.

### **High-Level CBOW NN Process**

Let's assume that we have a vast quantity of text about Jason Statham because we're tremendous fans of his. This will have all of the relevant words, the **vocabulary**, and context about the actor. The NN will predict which word _in the vocabulary_ will most likely succeed "Jason Statham is an."

The NN's input is a set of context words ("Jason Statham is an") within a set context window. We'll use the aforementioned four words as context. Although this is our context, we still search for the most relevant subsequent word from the entire vocabulary.

Although I won't discuss the architecture in excruciating mathematical detail (which is actually quite awesome), the general steps are as follows:

1. Input: the context within the given context window.

2. Each word in the context window can be represented by a one-hot encoded vector (from the first section above!). We multiply each context word's vector by an **embedding matrix, E**. The initial values in the E matrix are arbitrary and will be gradually adjusted over time during training to improve accuracy of our next-word prediction. This context vector-embedding matrix multiplication yields a context embedding (a vector) for each context word.

3. Let's now _average all of these context embeddings_ to get **one context embedding vector**. This vector will represent all of the context's semantic meaning into one neat format.

4. Through some more linear algebra that I refrain from diving too deep into, we multiply the **context embedding vector C** (with **dimension d)** by a **weight matrix** **W** with **dimensions d x V** (V is the size of the vocabulary) and add a **bias vector, b**. As a result, we get raw scores for each token in the vocabulary. The output here is a long vector of raw scores for each token in the vocabulary.

5. Now we apply the **softmax function** over all of these raw score values and end up with a **probability distribution (a probability vector, p)** of all the tokens. Each element of this probability vector tells us the probability of each token appearing after the context. In our example, the value with the highest probability should be "actor" so we get "Jason Statham is an actor."

![](https://miro.medium.com/1*flNGuS2C49tMmN0pg3CCtA.png)

Now, when training the CBOW model, it _does_ know the correct answer. The most important thing to understand here is that as the model generates flawed answers over and over again, it gradually adjusts the E and W matrices until it gets correct answers and learns which E and W parameters produce the optimal guesses for subsequent tokens.

At a high level, a **loss function** and an additional concept called stochastic gradient descent drive this learning. Again, later I'll link another post about these topics here. This is just a high-level understanding with not a lot of mathematical accuracy; it's okay for now. Consider this: every time you complete the Jason Statham sentence with nonsensical, incorrect words like "octopus" or "racecar" or "megalodon," part of your lunch gets taken away. But you _want_ your lunch. To minimize your losses, you change your thinking and reasoning skills to improve your guessing strategy, and you ultimately get better at guessing the word. Eventually, you minimize your lunch losses and make great guesses like "actor." The NN is trained properly when it can continually make great guesses like this for not just this Jason Statham sentence but also for any other sentence about Jason Statham because our vocabulary and documents cover _everything_ Jason Statham-related.

## Skip-Gram

> Motivating Question: The CBOW NN method does a pretty solid job of evaluating context. However, by averaging context vectors, CBOW treats all the context words within a context window equally? How can we improve our method's accuracy get better representations of semantic relationships?

In CBOW, we search for a target word and evaluate all the context words with equal attention because of the averaging of context embeddings into one holistic context embedding. Ideally, we should give more focus to each combination of our target token and each context token.

Fortunately, skip-gram does this! Instead of searching for a target word (which will ultimately be one of the words of our whole vocabulary), skip-gram **starts at a target word and learns to predict the context words.** Analyzing such vast quantities of target-context pairs enables the model to predict words based on contextual use.

This is critical because there are many words that can mean different things in different contexts; this is called polysemy. For example, "rock" could refer to either a rock in nature or a genre of music. Skip-gram improves upon CBOW and can deal with this issue.

## Transformer

This is essentially the final boss of NLP (for now). Instead of writing about it here, I redirect you to [my blog about how the transformer architecture understands input](https://medium.com/@amallya0523/how-an-llm-understands-input-the-math-under-the-hood-114ac69f96c6).

The transformer is a beautiful feat of NLP that effectively incorporates all the learnings of the above methods. Simply put, it learns about all the relationships between every word in your input and does this in different ways through different lenses, simultaneously (in parallel). This is called **multi-head attention**. Each head focuses on a different aspect of the input (grammar, syntax, parts of speech, etc.) and they all collectively contribute to an elegant numerical representation of the input, from which the LLM can predict the appropriate response. The primary difference between the transformer model and skip-gram is that skip-gram focuses on a smaller context window, whereas the transformer model can capture long-range dependencies.

## Concluding Thoughts

I hope Part I helped you understand the motivations behind some of the initial NLP developments/ideas and uncovered the utility of vector embeddings. I also hope Part II piqued your interest regarding the underlying mathematics of more advanced NLP methods.

More to come soon. Thanks for reading.